<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
    Do not modify this file directly. Instead, copy entries that you --><!--
    wish to modify from this file into core-site.xml and change them --><!--
    there. If core-site.xml does not already exist, create it. -->
<configuration xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xsi:noNamespaceSchemaLocation="configuration.xsd">
    <!--- global properties -->
    <property>
        <name>default.heap.size</name>
        <value>4096</value>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>The default heap size of cluster</en>
        </definition>
        <global>true</global>
        <description>
            <en>Default memory size for datanode, jobtracker and hbase master.</en>
        </description>
    </property>
    <property>
        <name>hadoop.extra.classpath</name>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>basic</group>
        <definition>
            <en>Extra Java CLASSPATH element</en>
        </definition>
        <global>true</global>
        <description>
            <en>Extra Java CLASSPATH elements. Will be appended to the value of HADOOP_CLASSPATH in hadoop-env.sh.</en>
        </description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <readonly>true</readonly>
        <value>/tmp/hadoop-${user.name}</value>
        <valuetype>String</valuetype>
        <group>filesystem</group>
        <definition>
            <en>The temp directory for hadoop</en>
        </definition>
        <description>
            <en>A base for other temporary directories.</en>
        </description>
    </property>
    <property>
        <name>hadoop.native.lib</name>
        <readonly>true</readonly>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>basic</group>
        <definition>
            <en>use the native hadoop libraries</en>
        </definition>
        <description>
            <en>Should native hadoop libraries, if present, be used.</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>network</group>
        <definition>
            <en>hadoop web filter</en>
        </definition>
        <description>
            <en>A comma separated list of class names. Each class in the list
                must extend org.apache.hadoop.http.FilterInitializer. The
                corresponding Filter will be initialized. Then, the Filter will be
                applied to all user viewing jsp and servlet web pages. The ordering
                of the list defines the ordering of the filters.</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping</name>
        <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
        <valuetype>Class</valuetype>
        <group>security</group>
        <definition>
            <en>hadoop user mapping</en>
        </definition>
        <description>
            <en>Class for user to group mapping (get groups for a given user)</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>security</group>
        <definition>
            <en>hadoop security authorization</en>
        </definition>
        <description>
            <en>Is service-level authorization enabled?</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>simple</value>
        <valuetype>Choose</valuetype>
        <chooselist>simple,kerberos</chooselist>
        <group>security</group>
        <definition>
            <en>hadoop security authorization level</en>
        </definition>
        <description>
            <en>possible values are simple (no authentication), and kerberos</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.token.service.use_ip</name>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>security</group>
        <definition>
            <en>Controls whether tokens always use IP addresses</en>
        </definition>
        <description>
            <en>
            Controls whether tokens always use IP addresses. DNS changes will not
            be detected if this option is enabled. Existing client connections
            that break will always reconnect to the IP of the original host. New
            clients will connect to the host's new IP but fail to locate a token.
            Disabling this option will allow existing and new clients to detect
            an IP change and continue to locate the new host's token.
        </en>
        </description>
    </property>
    <!-- <property> <name>hadoop.security.service.user.name.key</name> <value></value>
        <description>Name of the kerberos principal of the user that owns a given
        service daemon </description> </property> -->
    <property>
        <name>hadoop.workaround.non.threadsafe.getpwuid</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>security</group>
        <definition>
            <en>hadoop thread safe</en>
        </definition>
        <description>
            <en>Some operating systems or authentication modules are known to
  have broken implementations of getpwuid_r and getpwgid_r, such that these
  calls are not thread-safe. Symptoms of this problem include JVM crashes
  with a stack trace inside these functions. If your system exhibits this
  issue, enable this configuration parameter to include a lock around the
  calls as a workaround.
  An incomplete list of some systems known to have this issue is available
  at http://wiki.apache.org/hadoop/KnownBrokenPwuidImplementations</en>
        </description>
    </property>
    <property>
        <name>hadoop.kerberos.kinit.command</name>
        <readonly>true</readonly>
        <value>kinit</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>Kerberos credentials</en>
        </definition>
        <description>
            <en>Used to periodically renew Kerberos credentials when provided
  to Hadoop. The default setting assumes that kinit is in the PATH of users
  running the Hadoop client. Change this to the absolute path to kinit if this
  is not the case.</en>
        </description>
    </property>
    <!--- logging properties -->
    <property>
        <name>hadoop.logfile.size</name>
        <enable>false</enable>
        <value>10000000</value>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>log file size</en>
        </definition>
        <description>
            <en>The max size of each log file</en>
        </description>
    </property>
    <property>
        <name>hadoop.logfile.count</name>
        <enable>false</enable>
        <value>10</value>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>log file number</en>
        </definition>
        <description>
            <en>The max number of log files</en>
        </description>
    </property>
    <!-- i/o properties -->
    <property>
        <name>io.file.buffer.size</name>
        <value>4096</value>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>File buffer size</en>
        </definition>
        <description>
            <en>The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.</en>
        </description>
    <sectionname>basic</sectionname>
    <form>TextItem</form>
    <allowempty>true</allowempty>
    </property>
    <property>
        <name>io.bytes.per.checksum</name>
        <value>512</value>
        <intel_default>512</intel_default>
        <recommendation>4096</recommendation>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>Checksum size</en>
        </definition>
        <description>
            <en>The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size.</en>
        </description>
    <sectionname>ioconf</sectionname>
    <form>TextItem</form>
    <allowempty>false</allowempty>
    </property>
    <property>
        <name>dfs.datanode.protocol.client-request.client-verification-field.exists</name>
        <value>false</value>
        <recommendation>true</recommendation>
        <valuetype>Boolean</valuetype>
        <group>io</group>
        <definition>
            <en>checksum.client-verification-field</en>
        </definition>
        <description>
            <en>Allow DataNode to skip loading Checksum files.</en>
        </description>
        <allowempty>true</allowempty>
    </property>
    <property>
        <name>io.skip.checksum.errors</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>io</group>
        <definition>
            <en>Skip checksum errors</en>
        </definition>
        <description>
            <en>If true, when a checksum error is encountered while
  reading a sequence file, entries are skipped, instead of throwing an
  exception.</en>
        </description>
    <sectionname>ioconf</sectionname>
    <form>RadioGroupItem</form>
    <radios>true,false</radios>
    <allowempty>false</allowempty>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
        <valuetype>Class</valuetype>
        <group>perf</group>
        <definition>
            <en>Compression codecs</en>
        </definition>
        <description>
            <en>A list of the compression codec classes that can be used
               for compression/decompression.</en>
        </description>
    </property>
    <property>
        <name>io.serializations</name>
        <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
        <valuetype>Class</valuetype>
        <group>perf</group>
        <definition>
            <en>Io serializaions</en>
        </definition>
        <description>
            <en>A list of serialization classes that can be used for
  obtaining serializers and deserializers.
    </en>
        </description>
    </property>
    <!-- file system properties -->
    <property>
        <name>fs.default.name</name>
        <enable>false</enable>
        <value>file:///</value>
        <valuetype>String</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File System name</en>
        </definition>
        <global>true</global>
        <description>
            <en>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.
     </en>
        </description>
    </property>
    <property>
        <name>hadoop.namenode</name>
        <value></value>
        <allowempty>true</allowempty>
        <global>true</global>
        <valuetype>String</valuetype>
        <group>namenode</group>
        <definition>
            <en>NameNode name</en>
        </definition>
        <description>
            <en>Server name for namenode.</en>
        </description>
    </property>
    <property>
        <name>hadoop.namenode.port</name>
        <value>8020</value>
        <valuetype>String</valuetype>
        <group>namenode</group>
        <definition>
            <en>NameNode port</en>
        </definition>
        <global>true</global>
        <description>
            <en>Server port for namenode.</en>
        </description>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>0</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Interval to delete checkpoints</en>
        </definition>
        <description>
            <en>Number of minutes after which the checkpoints
      get deleted. If zero, the trash feature is disabled.</en>
        </description>
    </property>
    <property>
        <name>fs.file.impl</name>
        <value>org.apache.hadoop.fs.LocalFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for file implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for file: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.hdfs.impl</name>
        <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for hdfs implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for hdfs: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.webhdfs.impl</name>
        <value>org.apache.hadoop.hdfs.web.WebHdfsFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for webhdfs implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for webhdfs: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.s3.impl</name>
        <value>org.apache.hadoop.fs.s3.S3FileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for s3 implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for s3: uris.
    </en>
        </description>
    </property>
    <property>
        <name>fs.s3n.impl</name>
        <value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for s3n implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for s3n: (Native S3) uris.</en>
        </description>
    </property>
    <property>
        <name>fs.kfs.impl</name>
        <value>org.apache.hadoop.fs.kfs.KosmosFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for kfs implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for kfs: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.hftp.impl</name>
        <value>org.apache.hadoop.hdfs.HftpFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for hftp implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for hftp: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.hsftp.impl</name>
        <value>org.apache.hadoop.hdfs.HsftpFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>The FileSystem for hsftp</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for hsftp: uris</en>
        </description>
    </property>
    <property>
        <name>fs.ftp.impl</name>
        <value>org.apache.hadoop.fs.ftp.FTPFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for ftp implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for ftp: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.ramfs.impl</name>
        <value>org.apache.hadoop.fs.InMemoryFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for ramfs implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The FileSystem for ramfs: uris.</en>
        </description>
    </property>
    <property>
        <name>fs.har.impl</name>
        <value>org.apache.hadoop.fs.HarFileSystem</value>
        <valuetype>Class</valuetype>
        <group>filesystem</group>
        <definition>
            <en>File system for hadoop archives implementation</en>
        </definition>
        <implementation>org.apache.hadoop.fs.FileSystem</implementation>
        <description>
            <en>The filesystem for Hadoop archives.</en>
        </description>
    </property>
    <property>
        <name>fs.har.impl.disable.cache</name>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Cache hadoop archive instance</en>
        </definition>
        <description>
            <en>Don't cache 'har' filesystem instances.</en>
        </description>
    </property>
    <property>
        <name>fs.checkpoint.dir</name>
        <value>${hadoop.tmp.dir}/dfs/namesecondary</value>
        <intel_default>${hadoop.tmp.dir}/dfs/namesecondary</intel_default>
        <recommendation>/hadoop/namesecondary</recommendation>
        <valuetype>Directory</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Checkpoint directory</en>
        </definition>
        <description>
            <en>Determines where on the local filesystem the DFS secondary
      name node should store the temporary images to merge.
      If this is a comma-delimited list of directories then the image is
      replicated in all of the directories for redundancy.</en>
        </description>
    </property>
    <property>
        <name>fs.checkpoint.edits.dir</name>
        <value>${fs.checkpoint.dir}</value>
        <valuetype>Directory</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Checkpoint edit directory</en>
        </definition>
        <description>
            <en>Determines where on the local filesystem the DFS secondary
      name node should store the temporary edits to merge.
      If this is a comma-delimited list of directoires then the edits is
      replicated in all of the directoires for redundancy.
      Default value is same as fs.checkpoint.dir</en>
        </description>
    </property>
    <property>
        <name>fs.checkpoint.period</name>
        <value>3600</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Checkpoint period</en>
        </definition>
        <description>
            <en>The number of seconds between two periodic checkpoints.</en>
        </description>
    </property>
    <property>
        <name>fs.checkpoint.size</name>
        <value>67108864</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Edit log size to checkpoint</en>
        </definition>
        <description>
            <en>The size of the current edit log (in bytes) that triggers
       a periodic checkpoint even if the fs.checkpoint.period hasn't expired.</en>
        </description>
    </property>
    <property>
        <name>fs.s3.block.size</name>
        <value>67108864</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Block size to write to S3</en>
        </definition>
        <description>
            <en>Block size to use when writing files to S3.</en>
        </description>
    </property>
    <property>
        <name>fs.s3.buffer.dir</name>
        <value>${hadoop.tmp.dir}/s3</value>
        <valuetype>Directory</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Buffer file directory</en>
        </definition>
        <description>
            <en>Determines where on the local filesystem the S3 filesystem
  should store files before sending them to S3
  (or after retrieving them from S3).
</en>
        </description>
    </property>
    <property>
        <name>fs.s3.maxRetries</name>
        <value>4</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Maximum retries number to reading or writing file to S3</en>
        </definition>
        <description>
            <en>The maximum number of retries for reading or writing files to S3,
  before we signal failure to the application.
</en>
        </description>
    </property>
    <property>
        <name>fs.s3.sleepTimeSeconds</name>
        <value>10</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Seconds to sleep between every retries to reading or writing file to S3</en>
        </definition>
        <description>
            <en>The number of seconds to sleep between each S3 retry.
</en>
        </description>
    </property>
    <property>
        <name>fs.automatic.close</name>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>filesystem</group>
        <definition>
            <en>FileSystem auto close</en>
        </definition>
        <description>
            <en>By default, FileSystem instances are automatically closed at program
  exit using a JVM shutdown hook. Setting this property to false disables this
  behavior. This is an advanced option that should only be used by server applications
  requiring a more carefully orchestrated shutdown sequence.
</en>
        </description>
    </property>
    <property>
        <name>fs.s3n.block.size</name>
        <value>67108864</value>
        <valuetype>Integer</valuetype>
        <group>filesystem</group>
        <definition>
            <en>Block size</en>
        </definition>
        <description>
            <en>Block size to use when reading files using the Native S3
  filesystem (s3n: URIs).</en>
        </description>
    </property>
    <property>
        <name>local.cache.size</name>
        <value>10737418240</value>
        <valuetype>Integer</valuetype>
        <group>perf</group>
        <definition>
            <en>cache size</en>
        </definition>
        <description>
            <en>
            The limit on the size of cache you want to keep, set by default to
            10GB. This will act as a soft limit on the cache directory for out of
            band data.
            </en>
        </description>
    </property>
    <property>
        <name>io.seqfile.compress.blocksize</name>
        <value>1000000</value>
        <valuetype>Integer</valuetype>
        <group>io</group>
        <definition>
            <en>Minum block size for compression</en>
        </definition>
        <description>
            <en>The minimum block size for compression in block compressed
          SequenceFiles.
</en>
        </description>
    </property>
    <property>
        <name>io.seqfile.lazydecompress</name>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>io</group>
        <definition>
            <en>Lazy decompress</en>
        </definition>
        <description>
            <en>Should values of block-compressed SequenceFiles be decompressed
          only when necessary.
</en>
        </description>
    </property>
    <property>
        <name>io.seqfile.sorter.recordlimit</name>
        <value>1000000</value>
        <valuetype>Integer</valuetype>
        <group>io</group>
        <definition>
            <en>Limit number of records</en>
        </definition>
        <description>
            <en>The limit on number of records to be kept in memory in a spill
          in SequenceFiles.Sorter
</en>
        </description>
    </property>
    <property>
        <name>io.mapfile.bloom.size</name>
        <value>1048576</value>
        <valuetype>Integer</valuetype>
        <group>io</group>
        <definition>
            <en>The size of BloomFilters</en>
        </definition>
        <description>
            <en>The size of BloomFilter-s used in BloomMapFile. Each time this many
  keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter).
  Larger values minimize the number of filters, which slightly increases the performance,
  but may waste too much space if the total number of keys is usually much smaller
  than this number.
</en>
        </description>
    </property>
    <property>
        <name>io.mapfile.bloom.error.rate</name>
        <value>0.005</value>
        <valuetype>Float</valuetype>
        <group>io</group>
        <definition>
            <en>Rate of false positives in BloomFilter</en>
        </definition>
        <description>
            <en>The rate of false positives in BloomFilter-s used in BloomMapFile.
  As this value decreases, the size of BloomFilter-s increases exponentially. This
  value is the probability of encountering false positives (default is 0.5%).
</en>
        </description>
    </property>
    <property>
        <name>hadoop.util.hash.type</name>
        <value>murmur</value>
        <valuetype>Choose</valuetype>
        <chooselist>murmur,jenkins</chooselist>
        <group>basic</group>
        <definition>
            <en>Hash type</en>
        </definition>
        <description>
            <en>The default implementation of Hash. Currently this can take one of the
  two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.
</en>
        </description>
    </property>
    <!-- ipc properties -->
    <property>
        <name>ipc.client.idlethreshold</name>
        <value>4000</value>
        <valuetype>Integer</valuetype>
        <group>ipc</group>
        <definition>
            <en>Connection threshold</en>
        </definition>
        <description>
            <en>Defines the threshold number of connections after which
               connections will be inspected for idleness.
</en>
        </description>
    </property>
    <property>
        <name>ipc.client.kill.max</name>
        <value>10</value>
        <valuetype>Integer</valuetype>
        <group>ipc</group>
        <definition>
            <en>Maximum clients number</en>
        </definition>
        <description>
            <en>Defines the maximum number of clients to disconnect in one go.
</en>
        </description>
    </property>
    <property>
        <name>ipc.client.connection.maxidletime</name>
        <value>10000</value>
        <valuetype>Integer</valuetype>
        <group>ipc</group>
        <definition>
            <en>Maximum time for connection</en>
        </definition>
        <description>
            <en>The maximum time in msec after which a client will bring down the
               connection to the server.
</en>
        </description>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>10</value>
        <valuetype>Integer</valuetype>
        <group>ipc</group>
        <definition>
            <en>Maximum number retries</en>
        </definition>
        <description>
            <en>Indicates the number of retries a client will make to establish
               a server connection.
</en>
        </description>
    </property>
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>128</value>
        <valuetype>Integer</valuetype>
        <group>ipc</group>
        <definition>
            <en>Length of the server listen queue</en>
        </definition>
        <description>
            <en>Indicates the length of the listen queue for servers accepting
               client connections.
</en>
        </description>
    </property>
    <property>
        <name>ipc.server.tcpnodelay</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>ipc</group>
        <definition>
            <en>Turn on Nagle's algorithem</en>
        </definition>
        <description>
            <en>Turn on/off Nagle's algorithm for the TCP socket connection on
  the server. Setting to true disables the algorithm and may decrease latency
  with a cost of more/smaller packets.
</en>
        </description>
    </property>
        <property>
        <name>ipc.client.tcpnodelay</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>ipc</group>
        <definition>
            <en>Whether to Turn on Nagle's algorithem on the client</en>
        </definition>
        <description>
            <en>Turn on/off Nagle's algorithm for the TCP socket connection on
  the client. Setting to true disables the algorithm and may decrease latency
  with a cost of more/smaller packets.
  </en>
        </description>
    </property>
    <property>
        <name>webinterface.private.actions</name>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>network</group>
        <definition>
            <en>Web interfaces</en>
        </definition>
        <description>
            <en> If set to true, the web interfaces of JT and NN may contain
                actions, such as kill job, delete file, etc., that should
                not be exposed to public. Enable this option if the interfaces
                are only reachable by those who have the right authorization.
</en>
        </description>
    </property>
    <!-- Proxy Configuration -->
    <property>
        <name>hadoop.rpc.socket.factory.class.default</name>
        <value>org.apache.hadoop.net.StandardSocketFactory</value>
        <valuetype>Class</valuetype>
        <group>proxy</group>
        <definition>
            <en>Socketfactory Class</en>
        </definition>
        <description>
            <en> Default SocketFactory to use. This parameter is expected to be
    formatted as "package.FactoryClassName".
</en>
        </description>
    </property>
    <property>
        <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
        <value></value>
        <valuetype>Class</valuetype>
        <group>proxy</group>
        <definition>
            <en>Socketfactory Class to use to Connect to DFS</en>
        </definition>
        <description>
            <en> SocketFactory to use to connect to a DFS. If null or empty, use
    hadoop.rpc.socket.class.default. This socket factory is also used by
    DFSClient to create sockets to DataNodes.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.socks.server</name>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>proxy</group>
        <definition>
            <en>Address used by SocksSocketfactory </en>
        </definition>
        <description>
            <en> Address (host:port) of the SOCKS server to be used by the
    SocksSocketFactory.
</en>
        </description>
    </property>
    <!-- Rack Configuration -->
    <property>
        <name>topology.node.switch.mapping.impl</name>
        <value>org.apache.hadoop.net.ScriptBasedMapping</value>
        <valuetype>Class</valuetype>
        <implementation>org.apache.hadoop.net.DNSToSwitchMapping</implementation>
        <group>rack</group>
        <definition>
            <en>Topology node switch mapping implemention</en>
        </definition>
        <description>
            <en> The default implementation of the DNSToSwitchMapping. It
    invokes a script specified in topology.script.file.name to resolve
    node names. If the value for topology.script.file.name is not set, the
    default value of DEFAULT_RACK is returned for all node names.
</en>
        </description>
    </property>
    <property>
        <name>topology.script.file.name</name>
        <enable>false</enable>
        <value></value>
        <intel_default></intel_default>
        <recommendation>/usr/lib/intelcloud/rackmap.sh</recommendation>
        <valuetype>String</valuetype>
        <group>rack</group>
        <definition>
            <en>The script name to get NetworkTopology name</en>
        </definition>
        <readonly>true</readonly>
        <description>
            <en> The script name that should be invoked to resolve DNS names to
    NetworkTopology names. Example: the script would take host.foo.bar as an
    argument, and return /rack1 as the output.
</en>
        </description>
    </property>
    <property>
        <name>topology.script.number.args</name>
        <enable>false</enable>
        <value>100</value>
        <intel_default>100</intel_default>
        <recommendation>1</recommendation>
        <valuetype>Integer</valuetype>
        <group>rack</group>
        <definition>
            <en>The number of topology script args</en>
        </definition>
        <readonly>true</readonly>
        <description>
            <en> The max number of args that the script configured with
    topology.script.file.name should be run with. Each arg is an
    IP address.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.uid.cache.secs</name>
        <value>14400</value>
        <valuetype>Integer</valuetype>
        <group>security</group>
        <definition>
            <en>The timeout for cache from UID to UserName</en>
        </definition>
        <description>
            <en> NativeIO maintains a cache from UID to UserName. This is
  the timeout for an entry in that cache. </en>
        </description>
    </property>
    <!-- HTTP web-consoles Authentication -->
    <property>
        <name>hadoop.http.authentication.type</name>
        <value>simple</value>
        <!--<recommendation>${hadoop.security.authentication}</recommendation> -->
        <valuetype>Choose</valuetype>
        <chooselist>simple,kerberos,${hadoop.security.authentication}</chooselist>
        <group>security</group>
        <definition>
            <en>Authentication type for HTTP endpoint</en>
        </definition>
        <description>
            <en>
    Defines authentication used for Oozie HTTP endpoint.
    Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>36000</value>
        <valuetype>Integer</valuetype>
        <group>security</group>
        <definition>
            <en>Authentication token validity</en>
        </definition>
        <description>
            <en>
    Indicates how long (in seconds) an authentication token is valid before it has
    to be renewed.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.signature.secret</name>
        <value>hadoop</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The signature secret for signing the Authentication token</en>
        </definition>
        <description>
            <en>
    The signature secret for signing the authentication tokens.
    If not set a random secret is generated at startup time.
    The same secret should be used for JT/NN/DN/TT configurations.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The domain to store authentication token</en>
        </definition>
        <description>
            <en>
    The domain to use for the HTTP cookie that stores the authentication token.
    In order to authentiation to work correctly across all Hadoop nodes web-consoles
    the domain must be correctly set.
    IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings.
    For this setting to work properly all nodes, the cluster must be configured
    to generate URLs with hostname.domain names on it.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>true</value>
        <valuetype>Boolean</valuetype>
        <group>security</group>
        <definition>
            <en>Simple authentication anonymous allowance</en>
        </definition>
        <description>
            <en>
    Indicates if anonymous requests are allowed when using 'simple' authentication.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.kerberos.principal</name>
        <hide>true</hide>
        <type>1</type>
        <value>HTTP/localhost@LOCALHOST</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>Authentication principal</en>
        </definition>
        <description>
            <en> Indicates the Kerberos principal to be used for HTTP endpoint.
                The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO
                specification.
            </en>
        </description>
    </property>
    <property>
        <name>hadoop.http.authentication.kerberos.keytab</name>
        <hide>true</hide>
        <value>${user.home}/hadoop.keytab</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>Location of keytab file</en>
        </definition>
        <description>
            <en>
    Location of the keytab file with the credentials for the principal.
    Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
</en>
        </description>
    </property>
    <!-- ACL related -->
    <property>
        <name>hadoop.security.group.mapping.ldap.url</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The URL of the LDAP server</en>
        </definition>
        <description>
            <en>
    The URL of the LDAP server to use for resolving user groups when using
    the LdapGroupsMapping user to group mapping.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl</name>
        <readonly>true</readonly>
        <value>false</value>
        <valuetype>Boolean</valuetype>
        <group>security</group>
        <definition>
            <en>Use SSL connecting to LDAP server</en>
        </definition>
        <description>
            <en>
    Whether or not to use SSL when connecting to the LDAP server.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl.keystore</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>File path to the SSL keystore</en>
        </definition>
        <description>
            <en>
    File path to the SSL keystore that contains the SSL certificate required
    by the LDAP server.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl.keystore.password.file</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>File path containing the password of LDAP SSL keystore</en>
        </definition>
        <description>
            <en>
    The path to a file containing the password of the LDAP SSL keystore.
    IMPORTANT: This file should be readable only by the Unix user running
    the daemons.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.bind.user</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The user name to bind as when connecting to the LDAP server</en>
        </definition>
        <description>
            <en>
    The distinguished name of the user to bind as when connecting to the LDAP
    server. This may be left blank if the LDAP server supports anonymous binds.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.bind.password.file</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The path to a file containing the password of the bind user</en>
        </definition>
        <description>
            <en>
    The path to a file containing the password of the bind user.
    IMPORTANT: This file should be readable only by the Unix user running
    the daemons.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.base</name>
        <readonly>true</readonly>
        <value></value>
        <allowempty>true</allowempty>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The search base for the LDAP connection</en>
        </definition>
        <description>
            <en>
    The search base for the LDAP connection. This is a distinguished name,
    and will typically be the root of the LDAP directory.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.user</name>
        <readonly>true</readonly>
        <value>(&amp;(objectClass=user)(sAMAccountName={0}))</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>An filter to use when searching for LDAP users</en>
        </definition>
        <description>
            <en>
    An additional filter to use when searching for LDAP users. The default will
    usually be appropriate for Active Directory installations. If connecting to
    an LDAP server with a non-AD schema, this should be replaced with
    (&amp;(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to
    denote where the username fits into the filter.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.group</name>
        <readonly>true</readonly>
        <value>(objectClass=group)</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>An filter to use when searching for LDAP groups</en>
        </definition>
        <description>
            <en>
    An additional filter to use when searching for LDAP groups. This should be
    changed when resolving groups against a non-Active Directory installation.
    posixGroups are currently not a supported group class.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.member</name>
        <readonly>true</readonly>
        <value>member</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The attribute identifying the users that are members of the group</en>
        </definition>
        <description>
            <en>
    The attribute of the group object that identifies the users that are
    members of the group. The default will usually be appropriate for
    any LDAP installation.
</en>
        </description>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.group.name</name>
        <value>cn</value>
        <valuetype>String</valuetype>
        <group>security</group>
        <definition>
            <en>The attribute identifying the group name</en>
        </definition>
        <description>
            <en>
    The attribute of the group object that identifies the group name. The
    default will usually be appropriate for all LDAP systems.
</en>
        </description>
    </property><!--
    <property>
        <name>hdfs.namenode</name>
        <value></value>
        <valuetype>String</valuetype>
        <group>namenode</group>
        <definition>
            <en>NameNode</en>
        </definition>
        <sectionname>basic</sectionname>
        <description>
            <en>Host name or IP address of namenode</en>
        </description>
        <form>TextItem</form>
        <allowempty>true</allowempty>
    </property>
   -->
    <briefsection>
        <sectionname>basic</sectionname>
        <name_en>Hadoop Basic</name_en>
        <autoexpand>true</autoexpand>
        <showdescription>false</showdescription>
    </briefsection>
    <briefsection>
        <sectionname>ioconf</sectionname>
        <name_en>Hadoop IO</name_en>
        <autoexpand>true</autoexpand>
        <showdescription>false</showdescription>
    </briefsection>
    <group>
        <id>basic</id>
        <name_en>Basic Configuration</name_en>
        <description_en>Basic configurations that get Hadoop running.</description_en>
    </group>
    <group>
        <id>perf</id>
        <name_en>Performance</name_en>
        <description_en>Configurations that affect Hadoop's performance</description_en>
    </group>
    <group>
        <id>security</id>
        <name_en>Security</name_en>
        <description_en>Security configurations like Kerberos.</description_en>
    </group>
    <group>
        <id>network</id>
        <name_en>Network Setting</name_en>
        <description_en>Network Setting.</description_en>
    </group>
    <group>
        <id>filesystem</id>
        <name_en>File System</name_en>
        <description_en>File System configurations.</description_en>
    </group>
    <group>
        <id>namenode</id>
        <name_en>Namenode</name_en>
        <description_en>Configurations for HDFS Namenode.</description_en>
    </group>
    <group>
        <id>io</id>
        <name_en>IO Configuration</name_en>
        <description_en>IO Configuration</description_en>
    </group>
    <group>
        <id>ipc</id>
        <name_en>IPC</name_en>
        <description_en>IPC Configuration</description_en>
    </group>
    <group>
        <id>proxy</id>
        <name_en>Proxy Configuration</name_en>
        <description_en>Proxy Configuration</description_en>
    </group>
    <group>
        <id>rack</id>
        <name_en>Rack Configuration</name_en>
        <description_en>Rack Configuration</description_en>
    </group>
</configuration>
